# TOON vs Embeddings: Â¿Puede ayudar a ahorrar tokens?

## ğŸ¤” Â¿QuÃ© es TOON?

**TOON (Token-Oriented Object Notation)** es un formato de serializaciÃ³n mÃ¡s compacto que JSON, diseÃ±ado para reducir tokens en LLMs.

**Ahorro tÃ­pico:** 30-60% menos tokens vs JSON

**Ejemplo:**
```json
// JSON: 15,145 tokens
{
  "repositories": [
    {"name": "repo1", "stars": 100},
    {"name": "repo2", "stars": 200}
  ]
}

// TOON: 8,745 tokens (42% menos)
repositories: [
  {name: repo1, stars: 100},
  {name: repo2, stars: 200}
]
```

## âŒ Â¿Por quÃ© NO aplica directamente a Embeddings?

### 1. **Embeddings usan texto plano, no JSON**

Cuando generas embeddings, envÃ­as el **contenido del chunk como texto plano**:

```typescript
// Lo que se envÃ­a a OpenAI:
const embedding = await provider.generateEmbedding(
  chunk.content  // â† Texto plano, no JSON
);

// Ejemplo de chunk.content:
"El plan de gobierno propone mejorar la educaciÃ³n pÃºblica 
mediante la inversiÃ³n en infraestructura escolar y la 
capacitaciÃ³n de docentes..."
```

**TOON solo ayuda si estÃ¡s enviando datos estructurados (JSON)**, no texto narrativo.

### 2. **Los metadatos ya estÃ¡n optimizados**

Los metadatos se almacenan en la base de datos, no se envÃ­an al embedding:

```typescript
// Esto NO se envÃ­a al embedding:
metadata: {
  documentId: uuid,
  chunkIndex: 0,
  tokens: 450,
  pageNumber: 12,
  qualityScore: 0.85,
  keywords: ["educaciÃ³n", "infraestructura"],
  entities: ["Ministerio de EducaciÃ³n"]
}
```

Solo se envÃ­a `chunk.content` (texto plano).

## âœ… TÃ©cnicas que SÃ pueden reducir tokens en embeddings

### 1. **CompresiÃ³n de texto antes de chunking**

**Eliminar redundancias:**
- Espacios mÃºltiples â†’ espacio simple
- Saltos de lÃ­nea innecesarios
- Caracteres especiales repetidos

**Ya lo haces en `TextCleaner`**, pero se puede mejorar.

### 2. **Ajustar tamaÃ±o de chunks**

**Reducir `chunkSize` y `maxChunkSize`:**
```typescript
// Actual:
chunkSize: 400 tokens
maxChunkSize: 600 tokens

// MÃ¡s agresivo:
chunkSize: 300 tokens
maxChunkSize: 400 tokens
```

**Pros:** Menos tokens por embedding
**Contras:** MÃ¡s chunks, mÃ¡s llamadas a la API

### 3. **Eliminar contenido de baja calidad**

**Filtrar chunks con `qualityScore < 0.5`:**
```typescript
// No generar embeddings para chunks de baja calidad
if (qualityMetrics.qualityScore < 0.5) {
  continue; // Skip este chunk
}
```

**Ahorro:** Evitas generar embeddings inÃºtiles

### 4. **Usar batch embeddings**

**OpenAI soporta batch (hasta 2048 textos):**
```typescript
// En lugar de:
for (const chunk of chunks) {
  await provider.generateEmbedding(chunk.content);
}

// Usar:
await provider.generateBatch(
  chunks.map(c => c.content)
);
```

**Ahorro:** MÃ¡s eficiente, pero mismo nÃºmero de tokens

### 5. **Pre-procesar texto para reducir tokens**

**TÃ©cnicas de compresiÃ³n de texto:**
- Eliminar palabras vacÃ­as (stop words) - **NO recomendado** (pierde semÃ¡ntica)
- Abreviar tÃ©rminos comunes - **NO recomendado** (pierde significado)
- Normalizar formato - **âœ… Ya lo haces**

## ğŸ¯ RecomendaciÃ³n para tu caso

### **TOON NO es Ãºtil aquÃ­ porque:**
1. Embeddings requieren texto natural, no estructurado
2. Los metadatos no se envÃ­an al embedding
3. TOON es para JSON/estructuras, no narrativa

### **Mejores estrategias:**

#### **OpciÃ³n 1: Filtrar chunks de baja calidad** (MÃ¡s impacto)
```typescript
// En storeChunks(), antes de generar embedding:
if (qualityMetrics.qualityScore < 0.5) {
  this.logger.warn(`Skipping low-quality chunk ${chunk.chunkIndex}`);
  continue; // No generar embedding
}
```

**Ahorro estimado:** 10-20% menos embeddings (chunks filtrados)

#### **OpciÃ³n 2: Reducir tamaÃ±o de chunks** (Moderado)
```typescript
chunkingOptions: {
  chunkSize: 300,      // Reducir de 400
  maxChunkSize: 400,   // Reducir de 600
  overlapSize: 30      // Reducir de 50
}
```

**Ahorro estimado:** 25-30% menos tokens por chunk
**Trade-off:** MÃ¡s chunks totales

#### **OpciÃ³n 3: Usar batch embeddings** (Eficiencia)
```typescript
// Generar embeddings en batch
const texts = chunks.map(c => c.content);
const batchResult = await embeddingProvider.generateBatch(texts);
```

**Ahorro:** Mismo tokens, pero mÃ¡s rÃ¡pido y eficiente

## ğŸ“Š ComparaciÃ³n de estrategias

| Estrategia | Ahorro Tokens | Impacto Calidad | Dificultad |
|------------|---------------|----------------|-----------|
| TOON | âŒ 0% | N/A | N/A |
| Filtrar baja calidad | âœ… 10-20% | âš ï¸ Bajo | ğŸŸ¢ FÃ¡cil |
| Reducir chunk size | âœ… 25-30% | âš ï¸ Medio | ğŸŸ¢ FÃ¡cil |
| Batch embeddings | âœ… 0% (velocidad) | âœ… Sin impacto | ğŸŸ¡ Medio |
| CompresiÃ³n texto | âœ… 5-10% | âš ï¸ Bajo | ğŸŸ¡ Medio |

## ğŸ’¡ ConclusiÃ³n

**TOON no es Ãºtil para embeddings** porque:
- Embeddings usan texto plano, no JSON
- TOON es para estructuras de datos
- No hay JSON que optimizar en el flujo de embeddings

**Mejores opciones:**
1. âœ… **Filtrar chunks de baja calidad** (implementaciÃ³n fÃ¡cil, buen ahorro)
2. âœ… **Reducir tamaÃ±o de chunks** (si calidad lo permite)
3. âœ… **Usar batch embeddings** (mejor eficiencia)

Â¿Quieres que implemente alguna de estas estrategias?

